[
  {
    "id": 1,
    "model": "gpt-oss-20b",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 1",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 1,
    "model": "grok-4.1-fast",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 1",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 1,
    "model": "gemini-2.0-flash",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 1",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 2,
    "model": "gpt-oss-20b",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 2",
    "ratings": {
      "term_preservation": 0.4,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 2,
    "model": "grok-4.1-fast",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 2",
    "ratings": {
      "term_preservation": 0.3,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.0
    }
  },
  {
    "id": 2,
    "model": "gemini-2.0-flash",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 2",
    "ratings": {
      "term_preservation": 0.4,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.4
    }
  },
  {
    "id": 3,
    "model": "gpt-oss-20b",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 3",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.5
    }
  },
  {
    "id": 3,
    "model": "grok-4.1-fast",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 3",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.7,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 3,
    "model": "gemini-2.0-flash",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 3",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.4,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 4,
    "model": "gpt-oss-20b",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 4",
    "ratings": {
      "term_preservation": 0.4,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 4,
    "model": "grok-4.1-fast",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 4",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.5
    }
  },
  {
    "id": 4,
    "model": "gemini-2.0-flash",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 4",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.4
    }
  },
  {
    "id": 5,
    "model": "gpt-oss-20b",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 5",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.4,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 5,
    "model": "grok-4.1-fast",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 5",
    "ratings": {
      "term_preservation": 0.4,
      "language_naturalness": 0.7,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 5,
    "model": "gemini-2.0-flash",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 5",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.6,
      "meaning_accuracy": 0.4
    }
  },
  {
    "id": 6,
    "model": "gpt-oss-20b",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 6",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.0,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 6,
    "model": "grok-4.1-fast",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 6",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.5,
      "meaning_accuracy": 0.7
    }
  },
  {
    "id": 6,
    "model": "gemini-2.0-flash",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 6",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.5,
      "meaning_accuracy": 0.7
    }
  },
  {
    "id": 7,
    "model": "gpt-oss-20b",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 7",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.7
    }
  },
  {
    "id": 7,
    "model": "grok-4.1-fast",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 7",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.5
    }
  },
  {
    "id": 7,
    "model": "gemini-2.0-flash",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 7",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.5
    }
  },
  {
    "id": 8,
    "model": "gpt-oss-20b",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 8",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.5
    }
  },
  {
    "id": 8,
    "model": "grok-4.1-fast",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 8",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.7
    }
  },
  {
    "id": 8,
    "model": "gemini-2.0-flash",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 8",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.7
    }
  },
  {
    "id": 9,
    "model": "gpt-oss-20b",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 9",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.4,
      "meaning_accuracy": 0.3
    }
  },
  {
    "id": 9,
    "model": "grok-4.1-fast",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 9",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.3,
      "meaning_accuracy": 0.2
    }
  },
  {
    "id": 9,
    "model": "gemini-2.0-flash",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 9",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.2
    }
  },
  {
    "id": 10,
    "model": "gpt-oss-20b",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "тут будет текст перевода модели gpt-oss-20b для текста ID 10",
    "ratings": {
      "term_preservation": 0.6,
      "language_naturalness": 0.7,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 10,
    "model": "grok-4.1-fast",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "тут будет текст перевода модели grok-4.1-fast для текста ID 10",
    "ratings": {
      "term_preservation": 0.2,
      "language_naturalness": 0.6,
      "meaning_accuracy": 0.8
    }
  },
  {
    "id": 10,
    "model": "gemini-2.0-flash",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "тут будет текст перевода модели gemini-2.0-flash для текста ID 10",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.7,
      "meaning_accuracy": 0.4
    }
  }
]