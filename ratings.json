[
  {
    "id": 1,
    "model": "gpt-5",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "Доминирующие модели трансдукции последовательностей основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации энкодер-декодер. Лучшие по качеству модели также связывают энкодер и декодер через механизм внимания. Мы предлагаем новую простую архитектуру сети — Transformer, основанную исключительно на механизмах внимания и полностью обходящую рекуррентность и свертки. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, при этом они более параллелируемы и требуют значительно меньше времени на обучение. Наша модель достигает 28.4 BLEU на задаче WMT 2014 по переводу с английского на немецкий, превзойдя существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче WMT 2014 по переводу с английского на французский наша модель устанавливает новый показатель BLEU для одной модели на уровне передового — 41.8 после обучения примерно 3,5 дня на восьми ГПУ, что составляет лишь малую долю затрат на обучение по сравнению с лучшими моделями из литературы. Мы показываем, что Transformer хорошо обобщается на другие задачи, успешно применяя его к английскому конституентному разбору как при большом, так и при ограниченном объёме обучающих данных.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 1,
    "model": "grok-4.1-fast",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "Доминирующие модели трансдукции последовательностей основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации энкодер-декодер. Лучшие по производительности модели также соединяют энкодер и декодер через механизм внимания. Мы предлагаем новую простую архитектуру сети, Transformer, основанную исключительно на механизмах внимания, полностью отказываясь от рекуррентности и сверток. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, при этом более параллелизуемы и требуют значительно меньше времени на обучение. Наша модель достигает 28.4 BLEU на задаче перевода WMT 2014 с английского на немецкий, улучшая существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче перевода WMT 2014 с английского на французский наша модель устанавливает новый state-of-the-art показатель BLEU для одиночных моделей в 41.8 после обучения в течение 3.5 дней на восьми GPUs, что составляет малую долю затрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо обобщается на другие задачи, успешно применяя его к конституентному парсингу английского языка как с большим, так и с ограниченным объемом обучающих данных.",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 1,
    "model": "gemini-2.0-flash-lite",
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translated_text": "Доминирующие модели секвенциального преобразования основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации кодер-декодер. Лучшие модели также соединяют кодер и декодер посредством механизма внимания. Мы предлагаем новую простую архитектуру сети, Трансформер, основанную исключительно на механизмах внимания, полностью отказываясь от рекуррентности и сверток. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, будучи более параллелизуемыми и требуя значительно меньше времени на обучение. Наша модель достигает 28,4 BLEU на задаче перевода с английского на немецкий WMT 2014, улучшая существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче перевода с английского на французский WMT 2014 наша модель устанавливает новый лучший результат BLEU для одной модели — 41,8 после 3,5 дней обучения на восьми GPU, что составляет лишь малую часть затрат на обучение лучших моделей из литературы. Мы демонстрируем, что Трансформер хорошо обобщается на другие задачи, успешно применяя его к синтаксическому разбору английского языка как с большими, так и с ограниченными данными для обучения.",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 2,
    "model": "gpt-5",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "Мы представляем новую модель представления языка под названием BERT, которая означает Bidirectional Encoder Representations from Transformers.\nВ отличие от недавних моделей представления языка, BERT предназначен для предобучения глубоко двунаправленных представлений из неразмеченного текста за счет совместного учета левого и правого контекста во всех слоях.\nВ результате предобученная модель BERT может быть донастроена всего лишь одним дополнительным выходным слоем, чтобы создавать передовые модели для широкого диапазона задач, таких как ответы на вопросы и вывод по естественному языку, без существенных архитектурных изменений, специфичных для конкретной задачи.\nBERT концептуально прост и эмпирически мощен. Он достигает новых передовых результатов по одиннадцати задачам обработки естественного языка, включая доведение оценки GLUE до 80,5% (улучшение на 7,7 процентных пунктов), точность MultiNLI до 86,7% (улучшение на 4,6 процентных пунктов), F1 теста SQuAD v1.1 по задаче ответов на вопросы до 93,2 (улучшение на 1,5 процентных пунктов) и F1 теста SQuAD v2.0 до 83,1 (улучшение на 5,1 процентных пунктов).",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 2,
    "model": "grok-4.1-fast",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "Мы представляем новую модель представления языка под названием BERT, что означает Bidirectional Encoder Representations from Transformers. В отличие от недавних моделей представления языка, BERT разработан для предварительного обучения глубоких двунаправленных представлений из неразмеченного текста путем совместного обусловливания как левого, так и правого контекста во всех слоях. В результате предварительно обученная модель BERT может быть дообучена всего с одним дополнительным выходным слоем для создания моделей передового уровня для широкого спектра задач, таких как question answering и language inference, без существенных модификаций архитектуры, специфичных для задачи.\n\nBERT концептуально прост и эмпирически мощный. Он достигает новых результатов передового уровня на одиннадцати задачах обработки естественного языка, включая повышение балла GLUE до 80.5% (абсолютное улучшение на 7.7 процентных пункта), точности MultiNLI до 86.7% (абсолютное улучшение на 4.6%), Test F1 вопросно-ответной системы SQuAD v1.1 до 93.2 (абсолютное улучшение на 1.5 пункта) и Test F1 SQuAD v2.0 до 83.1 (абсолютное улучшение на 5.1 пункта).",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.7,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 2,
    "model": "gemini-2.0-flash-lite",
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translated_text": "Мы представляем новую модель представления языка под названием BERT, что расшифровывается как Bidirectional Encoder Representations from Transformers. В отличие от недавних моделей представления языка, BERT разработан для предварительного обучения глубоких двунаправленных представлений из неразмеченного текста, совместно обуславливая как левый, так и правый контекст во всех слоях. В результате предварительно обученная модель BERT может быть дообучена с помощью всего одного дополнительного выходного слоя для создания передовых моделей для широкого спектра задач, таких как ответы на вопросы и языковые выводы, без существенных изменений архитектуры, специфичных для конкретной задачи.\nBERT концептуально прост и эмпирически мощен. Он достигает новых передовых результатов по одиннадцати задачам обработки естественного языка, включая повышение показателя GLUE до 80,5% (абсолютное улучшение на 7,7%), точности MultiNLI до 86,7% (абсолютное улучшение на 4,6%), F1 на тестовой выборке SQuAD v1.1 для ответов на вопросы до 93,2 (абсолютное улучшение на 1,5 пункта) и F1 на тестовой выборке SQuAD v2.0 до 83,1 (абсолютное улучшение на 5,1 пункта).",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 3,
    "model": "gpt-5",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "Это исследование представляет подробный анализ модели обнаружения объектов YOLOv8, сосредотачивая внимание на ее архитектуре, методах обучения и улучшениях производительности по сравнению с предыдущими версиями, такими как YOLOv5. Ключевые инновации, включая базовую сеть CSPNet для улучшенного извлечения признаков, компонент FPN+PAN для более эффективного многомасштабного обнаружения объектов и переход к безякорному подходу, подробно рассмотрены. В работе рассматриваются показатели YOLOv8 на бенчмарках, таких как Microsoft COCO и Roboflow 100, подчеркивая его высокую точность и возможности работы в реальном времени на разнообразном аппаратном обеспечении. Дополнительно исследование изучает удобные для разработчиков улучшения YOLOv8, такие как единый пакет на Python и CLI, которые упрощают обучение и развертывание модели. В целом, это исследование позиционирует YOLOv8 как передовое решение в развивающейся области обнаружения объектов.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 3,
    "model": "grok-4.1-fast",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "Это исследование представляет подробный анализ модели обнаружения объектов YOLOv8, с акцентом на её архитектуру, техники обучения и улучшения производительности по сравнению с предыдущими версиями, такими как YOLOv5. Ключевые инновации, включая backbone CSPNet для улучшенного извлечения признаков, шею FPN+PAN для превосходного обнаружения объектов на нескольких масштабах и переход к anchor-free подходу, тщательно рассмотрены. Статья обзоривает производительность YOLOv8 на бенчмарках, таких как Microsoft COCO и Roboflow 100, подчёркивая её высокую точность и возможности работы в реальном времени на разнообразных аппаратных платформах. Кроме того, исследование изучает удобные для разработчиков улучшения YOLOv8, такие как унифицированный пакет Python и CLI, которые упрощают обучение и развертывание модели. В целом, это исследование позиционирует YOLOv8 как решение передового уровня в развивающейся области обнаружения объектов.",
    "ratings": {
      "term_preservation": 0.6,
      "language_naturalness": 0.7,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 3,
    "model": "gemini-2.0-flash-lite",
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translated_text": "В данном исследовании представлен детальный анализ модели обнаружения объектов YOLOv8, уделяя особое внимание её архитектуре, методам обучения и улучшению производительности по сравнению с предыдущими версиями, такими как YOLOv5. Тщательно рассмотрены ключевые инновации, включая CSPNet backbone для улучшенного извлечения признаков, FPN+PAN neck для превосходного обнаружения объектов в разных масштабах и переход к подходу без якорей (anchor-free). В статье анализируется производительность YOLOv8 на таких бенчмарках, как Microsoft COCO и Roboflow 100, подчеркивая её высокую точность и возможности работы в реальном времени на различных аппаратных платформах. Кроме того, исследование рассматривает улучшения YOLOv8, ориентированные на разработчиков, такие как унифицированный пакет Python и интерфейс командной строки (CLI), которые упрощают обучение и развертывание моделей. В целом, данное исследование позиционирует YOLOv8 как передовое решение в развивающейся области обнаружения объектов.",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 4,
    "model": "gpt-5",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "Системы на базе PX4 используют датчики для определения состояния аппарата (необходимо для стабилизации и обеспечения автономного управления). Состояния аппарата включают: положение/высоту, курс, скорость, воздушную скорость, ориентацию (attitude), скорости вращения вокруг различных осей, уровень заряда батареи и т. д.\n\nPX4 минимально требует гироскоп, акселерометр, магнетометр (компас) и барометр. GPS или другая система позиционирования необходима для включения всех автоматических режимов и некоторых вспомогательных режимов. Самолёты с фиксированным крылом и VTOL-аппараты должны дополнительно оснащаться датчиком воздушной скорости (крайне рекомендуется).\n\nМинимальный набор датчиков встроен в контроллеры полета серии Pixhawk (и может также присутствовать на других платформах контроллеров). Дополнительные/внешние датчики можно подключать к контроллеру.\n\nНиже представлены некоторые внешние датчики.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 4,
    "model": "grok-4.1-fast",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "Системы на базе PX4 используют датчики для определения состояния аппарата (необходимого для стабилизации и обеспечения автономного управления). Состояния аппарата включают: позицию/высоту, курс, скорость, воздушную скорость, ориентацию (attitude), угловые скорости вращения в разных направлениях, уровень заряда батареи и т. д.\n\nМинимально PX4 требуется гироскоп, акселерометр, магнитометр (компас) и барометр. GPS или другая система позиционирования необходимы для активации всех автоматических режимов и некоторых ассистированных режимов. Самолёты с фиксированным крылом и VTOL-аппараты должны дополнительно включать датчик воздушной скорости (настоятельно рекомендуется).\n\nМинимальный набор датчиков встроен в полётные контроллеры серии Pixhawk (и может быть также в других платформах контроллеров). Дополнительные/внешние датчики могут быть подключены к контроллеру.\n\nНиже мы описываем некоторые из внешних датчиков.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 4,
    "model": "gemini-2.0-flash-lite",
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translated_text": "Системы на базе PX4 используют датчики для определения состояния аппарата (необходимого для стабилизации и обеспечения автономного управления). К состояниям аппарата относятся: положение/высота, курс, скорость, воздушная скорость, ориентация (поза), угловые скорости в разных направлениях, уровень заряда батареи и т. д.\nPX4 минимально требует гироскоп, акселерометр, магнитометр (компас) и барометр. Для включения всех автоматических режимов и некоторых вспомогательных режимов требуется GPS или другая система позиционирования. Аппараты с фиксированным крылом и VTOL дополнительно должны включать датчик воздушной скорости (настоятельно рекомендуется).\nМинимальный набор датчиков встроен в контроллеры полета серии Pixhawk (а также может присутствовать на других платформах контроллеров). Дополнительные/внешние датчики могут быть подключены к контроллеру.\nНиже мы описываем некоторые из внешних датчиков.",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.9,
      "meaning_accuracy": 0.9
    }
  },
  {
    "id": 5,
    "model": "gpt-5",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "DJI Mobile SDK — набор средств разработки ПО, предназначенный для обеспечения разработчиков доступом к возможностям продукции DJI в области беспилотных летательных аппаратов (дронов) и портативных камер.\nSDK упрощает процесс разработки приложений, беря на себя функции более низкого уровня, такие как стабилизация полета, управление батареей, передача сигналов и связь.\nТаким образом, от разработчика не требуется специальное образование в области робототехники или встроенных систем, и он может сосредоточиться на приложении, в которое интегрируется продукт DJI.\nSDK включает:\n    • библиотеку/фреймворк, который можно импортировать в приложение под Android или iOS и который обеспечивает доступ к продуктам DJI \n    • симулятор летательного аппарата и инструмент визуализации \n    • отладчик и удалённый логгер для iOS \n    • образцы кода и обучающие материалы \n    • это руководство разработчика и документация по API \nЭто введение опишет, как SDK подключается к продуктам, какие функциональные возможности предоставляет SDK и базовую архитектуру SDK.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 5,
    "model": "grok-4.1-fast",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "DJI Mobile SDK — это набор средств разработки программного обеспечения, предназначенный для предоставления разработчикам доступа к возможностям воздушных аппаратов и портативных камер DJI. SDK упрощает процесс разработки приложений, беря на себя низкоуровневые функции, такие как стабилизация полета, управление аккумулятором, передача сигнала и связь. Таким образом, разработчику не требуется опыт в робототехнике или встроенных системах, и он может сосредоточиться на приложении, в которое интегрируется продукт DJI.\n\nSDK включает:\n    • библиотеку/фреймворк, которую можно импортировать в приложение для Android или iOS и которая предоставляет доступ к продуктам DJI\n    • симулятор воздушного аппарата и инструмент визуализации\n    • отладчик и удаленный логгер для iOS\n    • примеры кода и руководства\n    • это руководство разработчика и документацию по API\n\nЭто введение опишет, как SDK подключается к продуктам, какие функции предоставляет SDK, и вводную архитектуру SDK.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 5,
    "model": "gemini-2.0-flash-lite",
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translated_text": "DJI Mobile SDK — это комплект для разработки программного обеспечения, предназначенный для предоставления разработчикам доступа к возможностям летательных аппаратов и ручных камер DJI. SDK упрощает процесс разработки приложений, беря на себя низкоуровневые функции, такие как стабилизация полета, управление батареей, передача сигнала и связь. Таким образом, разработчику не требуется опыт в робототехнике или встраиваемых системах, и он может вместо этого сосредоточиться на приложении, в которое интегрируется продукт DJI.\nSDK включает:\n    • библиотеку/фреймворк, который можно импортировать в приложение для Android или iOS, предоставляющий доступ к продукту DJI\n    • авиационный симулятор и инструмент визуализации\n    • отладчик и удаленный регистратор для iOS\n    • примеры кода и руководства\n    • данное руководство разработчика и документацию по API\nВо введении будет описано, как SDK подключается к продуктам, какую функциональность предоставляет SDK и введение в архитектуру SDK.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 6,
    "model": "gpt-5",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "Прогрессы в области искусственного интеллекта (ИИ), включая большие языковые модели (LLMs) и гибридные модели рассуждений, создают возможность переосмыслить, как автономные роботы, такие как беспилотники, проектируются, развиваются и валидируются. Здесь мы демонстрируем полностью ИИ-генерируемую систему управления дроном: с минимальным участием человека искусственный интеллект (ИИ) написал весь код для платформы командования и управления дроном в реальном времени, размещаемой локально на дроне, которая была развёрнута и продемонстрирована на реальном дроне в полёте, а также на симулированном виртуальном дроне в облаке. Система обеспечивает картографирование в реальном времени, телеметрию полета, автономное планирование и выполнение миссий, а также протоколы безопасности — всё это управляется через веб-интерфейс, размещённый непосредственно на самом дроне. Ни одной строки кода не написал человек. Мы количественно сравнили производительность системы, сложность кода и скорость разработки с ранее существовавшими архитектурами, реализованными людьми, выяснив, что код, сгенерированный ИИ, может осуществлять функционально полноценные стеки командования и управления на порядки быстрее циклов разработки, хотя существуют явные ограничения, связанные с конкретным окном контекста модели и глубиной рассуждений. Наш анализ выявляет практические границы генерации кода управления роботами на текущих масштабах моделей, а также возникающие сильные стороны и режимы отказов в коде робототехники, сгенерированном ИИ. Эта работа устанавливает прецедент автономного создания систем управления роботами и, шире, намечает новую парадигму робототехнической инженерии — в которой будущие роботы могут быть во многом совместно спроектированы, разработаны и проверены искусственным интеллектом. В этой начальной работе робот построил мозг робота.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 6,
    "model": "grok-4.1-fast",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "Прогресс в области искусственного интеллекта (ИИ), включая большие языковые модели (LLM) и гибридные модели рассуждений, открывает возможности для переосмысления подходов к проектированию, разработке и валидации автономных роботов, таких как дроны. Здесь мы демонстрируем полностью сгенерированную ИИ систему управления дроном: с минимальным участием человека модель искусственного интеллекта (ИИ) создала весь код для платформы управления и командования дроном в реальном времени с самостоятельным хостингом, которая была развернута и продемонстрирована на реальном дроне в полете, а также на симулированном виртуальном дроне в облаке. Система обеспечивает картирование в реальном времени, телеметрию полета, автономное планирование и выполнение миссий, а также протоколы безопасности — все это оркестрируется через веб-интерфейс, размещенный непосредственно на самом дроне. Ни одной строки кода не написал человек. Мы количественно оцениваем производительность системы, сложность кода и скорость разработки по сравнению с предыдущими архитектурами, написанными человеком, и обнаруживаем, что код, сгенерированный ИИ, может обеспечивать функционально полные стеки командования и управления с циклами разработки на порядки быстрее, хотя и с выявленными текущими ограничениями, связанными с размером окна контекста конкретной модели и глубиной рассуждений. Наш анализ выявляет практические границы генерации кода управления роботом с помощью ИИ на текущих масштабах моделей, а также возникающие сильные стороны и режимы отказа в коде робототехники, сгенерированном ИИ. Эта работа устанавливает прецедент для автономного создания систем управления роботами и, в более широком смысле, предлагает новую парадигму робототехнической инженерии — ту, в которой будущие роботы в значительной степени будут совместно спроектированы, разработаны и верифицированы искусственным интеллектом. В этой начальной работе робот создал мозг робота.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 6,
    "model": "gemini-2.0-flash-lite",
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translated_text": "Достижения в области искусственного интеллекта (ИИ), включая большие языковые модели (LLM) и гибридные модели рассуждений, предоставляют возможность переосмыслить проектирование, разработку и валидацию автономных роботов, таких как дроны. Здесь мы демонстрируем полностью сгенерированную ИИ систему управления дронами: при минимальном участии человека модель искусственного интеллекта (ИИ) создала весь код для саморазмещаемой платформы командно-контрольных функций дрона в реальном времени, которая была развернута и продемонстрирована как на реальном дроне в полете, так и на симулированном виртуальном дроне в облаке. Система обеспечивает картографирование в реальном времени, телеметрию полета, автономное планирование и выполнение миссий, а также протоколы безопасности — всё это оркестровано через веб-интерфейс, размещенный непосредственно на самом дроне. Ни одна строка кода не была написана человеком. Мы количественно оцениваем производительность системы, сложность кода и скорость разработки по сравнению с предыдущими, написанными человеком архитектурами, и обнаруживаем, что сгенерированный ИИ код может обеспечивать функционально полные стеки командно-контрольных функций при циклах разработки, ускоренных на порядки, хотя и с выявляемыми текущими ограничениями, связанными с конкретным контекстным окном модели и глубиной рассуждений. Наш анализ раскрывает практические границы генерации кода управления роботами на основе ИИ в текущих масштабах моделей, а также возникающие сильные стороны и режимы отказа в сгенерированном ИИ коде для робототехники. Эта работа создает прецедент для автономного создания систем управления роботами и, в более широком смысле, предлагает новую парадигму для машиностроения в области робототехники — парадигму, в которой будущие роботы могут быть в значительной степени совместно спроектированы, разработаны и проверены искусственным интеллектом. В этой начальной работе робот построил мозг робота.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 7,
    "model": "gpt-5",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n    • Core functionality (core) - компактный модуль, определяющий базовые структуры данных, включая плотный многомерный массив Mat и базовые функции, используемые всеми остальными модулями.\n    • Image Processing (imgproc) - модуль обработки изображений, который включает линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (resize, affine and perspective warping, generic table-based remapping), преобразование цветовых пространств, гистограммы и т. д.\n    • Image file reading and writing (imgcodecs) - включает функции для чтения и записи изображений в различных форматах.\n    • Video I/O (videoio) - удобный интерфейс для захвата видео и кодеков.\n    • High-level GUI (highgui) - удобный интерфейс для простейших возможностей пользовательского интерфейса.\n    • Video Analysis (video) - модуль анализа видео, включающий оценку движения, вычитание фона и алгоритмы отслеживания объектов.\n    • Camera Calibration and 3D Reconstruction (calib3d) - базовые алгоритмы многовидовой геометрии, однокамерная и стереокалибровка, оценка позы объектов, алгоритмы стерео соответствий и элементы 3D-восстановления.\n    • 2D Features Framework (features2d) - детекторы признаков, дескрипторы и сопоставители дескрипторов.\n    • Object Detection (objdetect) - обнаружение объектов и экземпляров предопределённых классов (например, лица, глаза, кружки, люди, автомобили и т. п.).\n    • Deep Neural Network module (dnn) - модуль глубоких нейронных сетей.\n    • Machine Learning (ml) - Модуль машинного обучения включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n    • Computational Photography (photo) - передовые техники обработки фотографий, такие как удаление шума и восстановление пропусков.\n    • Images stitching (stitching) - функции для сшивания изображений и создания панорам.\n    • ... некоторые другие вспомогательные модули, такие как FLANN и обертки Google Test, привязки Python и другие.",
    "ratings": {
      "term_preservation": 0.6,
      "language_naturalness": 0.8,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 7,
    "model": "grok-4.1-fast",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n    • Основные функции (core) — компактный модуль, определяющий базовые структуры данных, включая плотный многомерный массив Mat и базовые функции, используемые всеми остальными модулями.\n    • Обработка изображений (imgproc) — модуль обработки изображений, включающий линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (изменение размера, аффинные и перспективные искажения, общее переmapping на основе таблиц), преобразование цветовых пространств, гистограммы и т. д.\n    • Чтение и запись файлов изображений (imgcodecs) — включает функции для чтения и записи файлов изображений в различных форматах.\n    • Ввод/вывод видео (videoio) — простой в использовании интерфейс для захвата видео и видеокодеков.\n    • GUI высокого уровня (highgui) — простой в использовании интерфейс для базовых возможностей UI.\n    • Анализ видео (video) — модуль анализа видео, включающий алгоритмы оценки движения, вычитания фона и отслеживания объектов.\n    • Калибровка камеры и 3D-реконструкция (calib3d) — базовые алгоритмы геометрии нескольких видов, калибровка одиночной и стереокамеры, оценка позы объекта, алгоритмы стереосоответствия и элементы 3D-реконструкции.\n    • Фреймворк 2D-особенностей (features2d) — детекторы заметных особенностей, дескрипторы и сопоставители дескрипторов.\n    • Обнаружение объектов (objdetect) — обнаружение объектов и экземпляров предопределенных классов (например, лица, глаза, кружки, люди, автомобили и т. д.).\n    • Модуль глубоких нейронных сетей (dnn) — модуль глубоких нейронных сетей.\n    • Машинное обучение (ml) — модуль машинного обучения включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n    • Вычислительная фотография (photo) — продвинутые техники обработки фото, такие как шумоподавление, инпеinting.\n    • Сшивание изображений (stitching) — функции для сшивания изображений и создания панорам.\n    • ... некоторые другие вспомогательные модули, такие как оболочки FLANN и Google test, привязки Python и другие.",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 7,
    "model": "gemini-2.0-flash-lite",
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translated_text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n    • Основная функциональность (core) - компактный модуль, определяющий основные структуры данных, включая плотный многомерный массив Mat, и основные функции, используемые всеми другими модулями.\n    • Обработка изображений (imgproc) - модуль обработки изображений, который включает линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (изменение размера, аффинные и перспективные преобразования, общее преобразование на основе таблиц), преобразование цветового пространства, гистограммы и т. д.\n    • Чтение и запись файлов изображений (imgcodecs) - включает функции для чтения и записи файлов изображений в различных форматах.\n    • Ввод-вывод видео (videoio) - простой в использовании интерфейс для захвата видео и кодеков видео.\n    • Высокоуровневый GUI (highgui) - простой в использовании интерфейс для базовых возможностей пользовательского интерфейса.\n    • Анализ видео (video) - модуль анализа видео, который включает алгоритмы оценки движения, вычитания фона и отслеживания объектов.\n    • Калибровка камеры и 3D-реконструкция (calib3d) - основные алгоритмы геометрии с несколькими видами, калибровка одиночной и стерео камеры, оценка позы объекта, алгоритмы стереосоответствия и элементы 3D-реконструкции.\n    • Структура 2D-признаков (features2d) - детекторы заметных признаков, дескрипторы и матчеры дескрипторов.\n    • Обнаружение объектов (objdetect) - обнаружение объектов и экземпляров предопределенных классов (например, лиц, глаз, кружек, людей, автомобилей и т. д.).\n    • Модуль глубоких нейронных сетей (dnn) - Модуль глубоких нейронных сетей.\n    • Машинное обучение (ml) - Модуль машинного обучения включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n    • Компьютерная фотография (photo) - передовые методы обработки фотографий, такие как шумоподавление, закрашивание.\n    • Сшивка изображений (stitching) - функции для сшивки изображений и создания панорам.\n    • ... некоторые другие вспомогательные модули, такие как FLANN и обертки Google test, привязки Python и другие.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 8,
    "model": "gpt-5",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "Обнаружение беспилотников выиграло от улучшений в глубоких нейронных сетях, но, как и во многих других приложениях, сталкивается с нехваткой точных данных для обучения. Синтетические данные обеспечивают потенциал для недорогой генерации данных и, как показывают результаты, улучшают доступность и качество данных. Однако модели, обученные на синтетических наборах данных, должны подтвердить свою способность работать с данными реального мира, что известно как проблема переноса из симуляции в реальность. Здесь мы представляем модель обнаружения беспилотников на основе Faster-RCNN, обученную на полностью синтетическом наборе данных, которая переносится на данные реального мира. Мы нашли, что она достигает AP_50 равного 97.0% при оценке на MAV-Vid — реальном наборе данных летающих беспилотников — по сравнению с 97.8% для эквивалентной модели, обученной на данных реального мира. Наши результаты показывают, что использование синтетических данных для обнаружения беспилотников может снизить затраты на сбор данных и повысить качество аннотирования. Эти выводы могут стать отправной точкой для разработки более продвинутых синтетических наборов данных дронов. Например, реалистичные воссоздания конкретных сценариев могут снизить риски при создании наборов данных для критически важных с точки зрения безопасности приложений, таких как обнаружение дронов в аэропортах. Более того, синтетические данные могут позволить создать надёжные системы обнаружения дронов, что может принести пользу и другим областям, таким как системы управления движением беспилотных летательных средств (UTM).",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 8,
    "model": "grok-4.1-fast",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "Обнаружение дронов выиграло от улучшений в глубоких нейронных сетях, но, как и многие другие приложения, страдает от недостатка точных данных для обучения. Синтетические данные предоставляют потенциал для генерации данных с низкими затратами и были показаны как способ улучшить доступность и качество данных. Однако модели, обученные на синтетических наборах данных, должны доказать свою способность работать на реальных данных — это известно как проблема sim-to-real transferability. Здесь мы представляем модель Faster-RCNN для обнаружения дронов, обученную на чисто синтетическом наборе данных, которая переносится на реальные данные. Мы обнаружили, что она достигает AP_50 в 97.0% при оценке на MAV-Vid — реальном наборе данных с летающими дронами — по сравнению с 97.8% для эквивалентной модели, обученной на реальных данных. Наши результаты показывают, что использование синтетических данных для обнаружения дронов имеет потенциал для снижения затрат на сбор данных и улучшения качества разметки. Эти выводы могут стать отправной точкой для более сложных синтетических наборов данных по дронам. Например, реалистичные воссоздания конкретных сценариев могли бы снизить риски при генерации наборов данных для критически важных для безопасности приложений, таких как обнаружение дронов в аэропортах. Кроме того, синтетические данные могут сделать возможными надежные системы обнаружения дронов, что могло бы принести пользу другим областям, таким как системы управления трафиком беспилотников.",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 8,
    "model": "gemini-2.0-flash-lite",
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translated_text": "Детектирование дронов получило преимущества благодаря усовершенствованию глубоких нейронных сетей, но, как и многие другие приложения, страдает от недостатка точных данных для обучения. Синтетические данные предоставляют возможность для недорогой генерации данных и доказали свою эффективность в улучшении доступности и качества данных. Однако модели, обученные на синтетических наборах данных, должны продемонстрировать свою способность работать на реальных данных, что известно как проблема переноса из симуляции в реальность (sim-to-real transferability). Здесь мы представляем модель Faster-RCNN для детектирования дронов, обученную на полностью синтетическом наборе данных, которая переносится на реальные данные. Мы обнаружили, что она достигает AP_50 97,0% при оценке на MAV-Vid — реальном наборе данных летающих дронов — по сравнению с 97,8% для эквивалентной модели, обученной на реальных данных. Наши результаты показывают, что использование синтетических данных для детектирования дронов может снизить затраты на сбор данных и повысить качество разметки. Эти выводы могут послужить отправной точкой для создания более сложных синтетических наборов данных о дронах. Например, реалистичное воссоздание конкретных сценариев может снизить риски при генерации данных для критически важных приложений, таких как обнаружение дронов в аэропортах. Кроме того, синтетические данные могут обеспечить надежные системы обнаружения дронов, которые могут принести пользу другим областям, таким как системы управления воздушным движением беспилотников.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 0.9,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 9,
    "model": "gpt-5",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "Способность беспилотных летательных аппаратов к эффективной автономной навигации и обходу препятствий в сложных и неизвестных условиях среды критически важна для применений в сельскохозяйственном орошении, ликвидации последствий стихийных бедствий и логистике. В настоящей работе мы предлагаем алгоритм навигации DPRL (Distributed Privileged Reinforcement Learning) — энд-энд-политика, предназначенная для решения проблемы высокоскоростной автономной навигации БПЛА в условиях частично наблюдаемой окружающей среды. Наш подход объединяет глубокое обучение с подкреплением и привилегированное обучение, чтобы преодолеть влияние искажения данных наблюдений, вызванного частичной наблюдаемостью. Мы используем асимметричную архитектуру Actor-Critic, чтобы предоставлять агенту привилегированную информацию во время обучения, что повышает способности восприятия модели. Кроме того, мы предлагаем мультиагентную стратегию исследования в разнообразных средах для ускорения сбора опыта, что, в свою очередь, ускоряет сходимость модели. Мы провели обширные симуляции в различных сценариях, сравнив наш алгоритм DPRL с современными алгоритмами навигации. Результаты последовательно демонстрируют превосходную производительность нашего алгоритма по таким критериям, как эффективность полета, устойчивость и общий уровень успешности.",
    "ratings": {
      "term_preservation": 0.7,
      "language_naturalness": 0.8,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 9,
    "model": "grok-4.1-fast",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "Способность БПЛА к эффективной автономной навигации и избеганию препятствий в сложных и неизвестных средах критически важна для приложений в сельскохозяйственном орошении, ликвидации последствий катастроф и логистике. В этой статье мы предлагаем алгоритм навигации DPRL (Distributed Privileged Reinforcement Learning) — end-to-end политику, разработанную для решения задачи высокоскоростной автономной навигации БПЛА в условиях частично наблюдаемой окружающей среды. Наш подход сочетает deep reinforcement learning с privileged learning, чтобы преодолеть влияние повреждения данных наблюдений, вызванного частичной наблюдаемостью. Мы используем асимметричную архитектуру Actor-Critic, чтобы предоставить агенту привилегированную информацию во время обучения, что повышает перцептивные возможности модели. Кроме того, мы представляем многозагентную стратегию исследования в разнообразных средах для ускорения сбора опыта, что, в свою очередь, ускоряет сходимость модели. Мы провели обширные симуляции в различных сценариях, сравнивая наш алгоритм DPRL с передовыми алгоритмами навигации. Результаты последовательно демонстрируют превосходную производительность нашего алгоритма по критериям эффективности полета, устойчивости и общего уровня успеха.",
    "ratings": {
      "term_preservation": 0.5,
      "language_naturalness": 0.7,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 9,
    "model": "gemini-2.0-flash-lite",
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translated_text": "Способность БПЛА к эффективной автономной навигации и обходу препятствий в сложных и неизвестных средах имеет решающее значение для таких применений, как орошение сельскохозяйственных угодий, ликвидация последствий стихийных бедствий и логистика. В данной статье мы предлагаем алгоритм навигации DPRL (Distributed Privileged Reinforcement Learning) — сквозную политику, разработанную для решения задачи высокоскоростной автономной навигации БПЛА в условиях частичной наблюдаемости окружающей среды. Наш подход сочетает глубокое обучение с подкреплением и привилегированное обучение для преодоления влияния искажения данных наблюдений, вызванного частичной наблюдаемостью. Мы используем асимметричную архитектуру Actor-Critic, чтобы предоставить агенту привилегированную информацию во время обучения, что улучшает перцептивные способности модели. Кроме того, мы представляем стратегию исследования с использованием нескольких агентов в разнообразных средах для ускорения сбора опыта, что, в свою очередь, ускоряет сходимость модели. Мы провели обширные симуляции в различных сценариях, сравнивая наш алгоритм DPRL с передовыми навигационными алгоритмами. Результаты последовательно демонстрируют превосходную производительность нашего алгоритма с точки зрения эффективности полета, надежности и общей частоты успешных выполнений.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 10,
    "model": "gpt-5",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "В последние годы растущее использование беспилотников как в коммерческих, так и в любительских целях привело к возросшей озабоченности безопасностью воздушного пространства. Для решения этих проблем возникла детекция и классификация беспилотников на основе машинного обучения (ML). Это исследование изучает потенциал классификации беспилотников на основе машинного обучения, с использованием таких технологий, как радар, визуальные, акустические и радиочастотные сенсорные системы. Оно проводит всестороннее изучение существующей литературы в этой области, сосредоточив внимание на различных сенсорных модальностях и их соответствующих технологических реализациях. Исследование указывает на перспективность классификации беспилотников на основе ML, при этом имеется множество успешных отдельных работ. Однако следует отметить, что большая часть исследований в этой области экспериментальна, что затрудняет сопоставление результатов между различными статьями. Кроме того, отмечается заметный дефицит эталонных наборов данных, помогающих в оценке различных решений.",
    "ratings": {
      "term_preservation": 0.8,
      "language_naturalness": 0.8,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 10,
    "model": "grok-4.1-fast",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "В последние годы растущая популярность использования дронов в коммерческих и рекреационных целях привела к усилению опасений по поводу безопасности воздушного пространства. Для решения этих проблем появились методы обнаружения и классификации дронов на основе machine learning (ML). Это исследование изучает потенциал классификации дронов на основе ML с использованием технологий вроде радара, визуального, акустического и радиочастотного сенсинга. Оно проводит всесторонний обзор существующей литературы в этой области с акцентом на различные модальности сенсинга и их соответствующие технологические реализации. Исследование показывает, что классификация дронов на основе ML перспективна и включает многочисленные успешные индивидуальные вклады. Однако важно отметить, что большая часть исследований в этой области экспериментальная, что затрудняет сравнение результатов из различных статей. Также примечательно отсутствие эталонных датасетов для оценки различных решений.",
    "ratings": {
      "term_preservation": 0.9,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  },
  {
    "id": 10,
    "model": "gemini-2.0-flash-lite",
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translated_text": "В последние годы участившееся использование дронов как в коммерческих, так и в рекреационных целях привело к повышенной обеспокоенности по поводу безопасности воздушного пространства. Для решения этих проблем появились системы обнаружения и классификации дронов на основе машинного обучения (ML). Данное исследование изучает потенциал классификации дронов на основе ML с использованием таких технологий, как радарные, визуальные, акустические и радиочастотные системы зондирования. Оно проводит всестороннее изучение существующей литературы в этой области с акцентом на различные модальности зондирования и их соответствующие технологические реализации. Исследование показывает, что классификация дронов на основе ML является перспективной, с многочисленными успешными индивидуальными вкладами. Однако важно отметить, что большая часть исследований в этой области является экспериментальной, что затрудняет сравнение результатов из различных статей. Также заметно отсутствие эталонных наборов данных, которые могли бы помочь в оценке различных решений.",
    "ratings": {
      "term_preservation": 1.0,
      "language_naturalness": 1.0,
      "meaning_accuracy": 1.0
    }
  }
]