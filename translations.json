[
  {
    "id": 1,
    "original_text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Доминирующие модели преобразования последовательностей основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации энкодер‑декодер. Лучшие по качеству модели также связывают энкодер и декодер через механизм внимания. Мы предлагаем новую простую архитектуру сети — Transformer, основанную исключительно на механизмах внимания, полностью исключающую рекуррентность и свёртки. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, при этом они более параллелизируемы и требуют значительно меньше времени на обучение. Наша модель достигает 28,4 BLEU на задаче перевода WMT 2014 с английского на немецкий, превосходя существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче перевода WMT 2014 с английского на французский язык наша модель устанавливает новый рекорд BLEU для одиночной модели — 41,8 после обучения в течение 3,5 суток на восьми GPU, что составляет лишь малую долю затрат на обучение по сравнению с лучшими моделями из литературы. Мы показываем, что Transformer хорошо обобщается на другие задачи, успешно применяя его к английскому конституентному разбору как с большим, так и с ограниченным обучающим набором данных."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Доминирующие модели преобразования последовательностей основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации энкодер-декодер. Лучшие по производительности модели также соединяют энкодер и декодер через механизм внимания. Мы предлагаем новую простую архитектуру сети, Transformer, основанную исключительно на механизмах внимания, полностью отказываясь от рекуррентности и сверток. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, при этом более параллелизуемы и требуют значительно меньше времени на обучение. Наша модель достигает 28.4 BLEU на задаче перевода с английского на немецкий в WMT 2014, улучшая существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче перевода с английского на французский в WMT 2014 наша модель устанавливает новый рекорд BLEU для одиночной модели в 41.8 после обучения в течение 3.5 дней на eight GPUs, что составляет малую долю затрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо обобщается на другие задачи, успешно применяя его к конституентному разбору английского языка как с большими, так и с ограниченными наборами данных для обучения."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Доминирующие модели секвенциального преобразования основаны на сложных рекуррентных или сверточных нейронных сетях в конфигурации энкодер-декодер. Наиболее эффективные модели также связывают энкодер и декодер через механизм внимания. Мы предлагаем новую простую архитектуру сети, Трансформер, основанную исключительно на механизмах внимания, полностью отказываясь от рекуррентности и сверток. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят существующие по качеству, будучи более параллелизуемыми и требуя значительно меньше времени на обучение. Наша модель достигает 28,4 BLEU в задаче перевода с английского на немецкий WMT 2014, улучшая существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. В задаче перевода с английского на французский WMT 2014 наша модель устанавливает новый лучший результат для отдельной модели BLEU, равный 41,8, после обучения в течение 3,5 дней на восьми GPU, что составляет лишь малую часть затрат на обучение лучших моделей из литературы. Мы показываем, что Трансформер хорошо обобщается на другие задачи, успешно применяя его к анализу синтаксической структуры английского языка как с большими, так и с ограниченными данными для обучения."
      }
    ]
  },
  {
    "id": 2,
    "original_text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Мы представляем новую языковую модель представления под названием BERT, расшифровывающуюся как Bidirectional Encoder Representations from Transformers. В отличие от недавних моделей представления языка, BERT спроектирована для предобучения глубоко двунаправленных представлений из неразмеченного текста, одновременно учитывая левый и правый контекст во всех слоях. В результате предобученная модель BERT может быть донастроена добавлением всего одного выходного слоя, чтобы создавать передовые модели для широкого диапазона задач, таких как вопросно-ответные системы и вывод по естественному языку, без существенных изменений архитектуры, специфичных для задачи. BERT концептуально проста и эмпирически мощна. Она достигает новых передовых результатов по одиннадцати задачам обработки естественного языка, включая повышение балла GLUE до 80,5% (абсолютное улучшение на 7,7 п.п.), точность MultiNLI до 86,7% (4,6 п.п. абсолютного улучшения), F1 на тесте SQuAD v1.1 по задаче вопросно-ответной до 93,2 (1,5 п.п. абсолютного улучшения) и F1 на тесте SQuAD v2.0 до 83,1 (5,1 п.п. абсолютного улучшения)."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Мы представляем новую модель представления языка под названием BERT, что означает Bidirectional Encoder Representations from Transformers. В отличие от недавних моделей представления языка, BERT предназначена для предварительного обучения глубоких двунаправленных представлений из немаркированного текста путём совместного обусловливания на левый и правый контекст во всех слоях. В результате предварительно обученная модель BERT может быть дообучена с помощью всего одного дополнительного выходного слоя для создания передовых моделей для широкого спектра задач, таких как question answering и language inference, без существенных модификаций архитектуры, специфичных для задачи.\n\nBERT концептуально прост и эмпирически мощён. Он достигает новых передовых результатов на одиннадцати задачах обработки естественного языка, включая повышение балла GLUE до 80,5% (абсолютное улучшение на 7,7 процентных пункта), точности MultiNLI до 86,7% (абсолютное улучшение на 4,6 процентных пункта), F1 Test question answering SQuAD v1.1 до 93,2 (абсолютное улучшение на 1,5 пункта) и F1 Test SQuAD v2.0 до 83,1 (абсолютное улучшение на 5,1 пункта)."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Мы представляем новую модель представления языка под названием BERT, что означает Bidirectional Encoder Representations from Transformers (двунаправленные представления кодировщика из трансформеров). В отличие от недавних моделей представления языка, BERT предназначен для предварительного обучения глубоких двунаправленных представлений из неразмеченного текста, совместно обуславливая как левый, так и правый контекст во всех слоях. В результате предварительно обученную модель BERT можно дообучить с помощью всего одного дополнительного выходного слоя для создания передовых моделей для широкого спектра задач, таких как ответы на вопросы и языковые выводы, без существенных модификаций архитектуры, специфичных для конкретной задачи.\nBERT концептуально прост и эмпирически мощен. Он демонстрирует новые передовые результаты в одиннадцати задачах обработки естественного языка, включая повышение показателя GLUE до 80,5% (абсолютное улучшение на 7,7%), точности MultiNLI до 86,7% (абсолютное улучшение на 4,6%), F1 на тестовом наборе SQuAD v1.1 для ответов на вопросы до 93,2 (абсолютное улучшение на 1,5 пункта) и F1 на тестовом наборе SQuAD v2.0 до 83,1 (абсолютное улучшение на 5,1 пункта)."
      }
    ]
  },
  {
    "id": 3,
    "original_text": "This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8’s performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8’s developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Это исследование представляет детальный анализ модели обнаружения объектов YOLOv8, сосредотачиваясь на её архитектуре, методах обучения и улучшениях производительности по сравнению с предыдущими итерациями, такими как YOLOv5. Ключевые инновации, включая базовую сеть CSPNet для улучшенного извлечения признаков, шея FPN+PAN для превосходного обнаружения объектов на разных масштабах и переход к безякорному подходу, подробно рассматриваются. В работе оцениваются показатели YOLOv8 на наборах данных Microsoft COCO и Roboflow 100, подчеркивая его высокую точность и возможности работы в реальном времени на разнообразной аппаратуре. Кроме таго, исследование рассматривает удобства для разработчиков YOLOv8, такие как единый пакет Python и интерфейс командной строки, которые упрощают обучение модели и её развертывание. В целом, данное исследование позиционирует YOLOv8 как передовое решение в развивающейся области обнаружения объектов."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Это исследование представляет подробный анализ модели обнаружения объектов YOLOv8, с акцентом на её архитектуру, техники обучения и улучшения производительности по сравнению с предыдущими версиями, такими как YOLOv5. Ключевые инновации, включая backbone CSPNet для улучшенного извлечения признаков, neck FPN+PAN для превосходного мультимасштабного обнаружения объектов и переход к anchor-free подходу, тщательно рассмотрены. Статья обзорит производительность YOLOv8 на бенчмарках, таких как Microsoft COCO и Roboflow 100, подчёркивая её высокую точность и возможности работы в реальном времени на разнообразных аппаратных платформах. Кроме того, исследование изучает удобные для разработчиков улучшения YOLOv8, такие как унифицированный Python-пакет и CLI, которые упрощают обучение и развертывание модели. В целом, это исследование позиционирует YOLOv8 как state-of-the-art решение в развивающейся области обнаружения объектов."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "В данном исследовании представлен детальный анализ модели обнаружения объектов YOLOv8, сосредоточенный на ее архитектуре, методах обучения и улучшениях производительности по сравнению с предыдущими версиями, такими как YOLOv5. Подробно рассматриваются ключевые инновации, включая CSPNet backbone для улучшенного извлечения признаков, FPN+PAN neck для превосходного обнаружения объектов в различных масштабах и переход к подходу без якорей (anchor-free). В статье анализируется производительность YOLOv8 на таких бенчмарках, как Microsoft COCO и Roboflow 100, подчеркивается ее высокая точность и возможности работы в реальном времени на различных аппаратных платформах. Кроме того, исследование затрагивает улучшения YOLOv8, ориентированные на разработчиков, такие как унифицированный Python-пакет и CLI, которые упрощают обучение и развертывание моделей. В целом, данное исследование позиционирует YOLOv8 как передовое решение в развивающейся области обнаружения объектов."
      }
    ]
  },
  {
    "id": 4,
    "original_text": "PX4-based systems use sensors to determine vehicle state (needed for stabilization and to enable autonomous control). The vehicle states include: position/altitude, heading, speed, airspeed, orientation (attitude), rates of rotation in different directions, battery level, and so on.\nPX4 minimally requires a gyroscope, accelerometer, magnetometer (compass) and barometer. A GPS or other positioning system is needed to enable all automatic modes, and some assisted modes. Fixed-wing and VTOL-vehicles should additionally include an airspeed sensor (highly recommended).\nThe minimal set of sensors is incorporated into Pixhawk Series flight controllers (and may also be in other controller platforms). Additional/external sensors can be attached to the controller.\nBelow we describe some of the external sensors.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Системы на базе PX4 используют датчики для определения состояния летательного аппарата (необходимо для стабилизации и обеспечения автономного управления). Состояния аппарата включают: положение/высоту, курс, скорость, скорость по воздуху, ориентацию (положение тела), угловые скорости вращения вокруг разных осей, уровень заряда батареи и т. д.\nPX4 минимально требует гироскоп, акселерометр, магнитометр (магнитометр) и барометр. GPS или другая система определения положения необходима для включения всех автоматических режимов, а также некоторых ассистируемых режимов. Летательные аппараты с фиксированным крылом и VTOL-аппараты следует дополнительно оснащать датчиком скорости по воздуху (очень рекомендуется).\nМинимальный набор сенсоров реализован в контроллерах Pixhawk Series (и может également присутствовать на других платформах контроллеров). Дополнительные внешние сенсоры можно подключить к контроллеру.\nНиже описаны некоторые внешние датчики."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Системы на базе PX4 используют датчики для определения состояния аппарата (необходимого для стабилизации и обеспечения автономного управления). Состояния аппарата включают: позицию/высоту, курс, скорость, воздушную скорость, ориентацию (attitude), угловые скорости вращения в разных направлениях, уровень заряда батареи и т. д.\n\nPX4 минимально требует гироскоп, акселерометр, магнитометр (компас) и барометр. GPS или другая система позиционирования необходимы для активации всех автоматических режимов и некоторых вспомогательных режимов. Самолёты с фиксированным крылом и VTOL-аппараты должны дополнительно включать датчик воздушной скорости (настоятельно рекомендуется).\n\nМинимальный набор датчиков встроен в полётные контроллеры серии Pixhawk (и может быть также в других платформах контроллеров). Дополнительные/внешние датчики могут быть подключены к контроллеру.\n\nНиже мы описываем некоторые из внешних датчиков."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Системы на базе PX4 используют датчики для определения состояния летательного аппарата (необходимого для стабилизации и обеспечения автономного управления). К состояниям летательного аппарата относятся: положение/высота, курс, скорость, скорость воздушного потока, ориентация (положение), угловые скорости в различных направлениях, уровень заряда батареи и т. д.\nPX4 минимально требует гироскоп, акселерометр, магнитометр (компас) и барометр. Для включения всех автоматических режимов и некоторых вспомогательных режимов необходима система GPS или другая система позиционирования. Для летательных аппаратов с неподвижным крылом и VTOL дополнительно следует использовать датчик воздушной скорости (настоятельно рекомендуется).\nМинимальный набор датчиков встроен в полетные контроллеры серии Pixhawk (а также может присутствовать на других платформах контроллеров). Дополнительные/внешние датчики могут быть подключены к контроллеру.\nНиже описаны некоторые из внешних датчиков."
      }
    ]
  },
  {
    "id": 5,
    "original_text": "The DJI Mobile SDK is a software development kit designed to give developers access to the capability of DJI's aircraft and handheld camera products. The SDK simplifies the application development process by taking care of lower level functionality such as flight stabilization, battery management, signal transmission and communication. This way, the developer does not require a background in robotics or embedded systems and can instead focus on the application the DJI product is being integrated into.\nThe SDK includes:\n    • a library/framework that can be imported into an Android or iOS app that give access to the DJI product \n    • an aircraft simulator and visualization tool \n    • debugger and remote logger for iOS \n    • sample code and tutorials \n    • this developer guide and API documentation \nThis introduction will describe how the SDK connects to the products, what functionality the SDK provides, and an introductory architecture of the SDK.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "DJI Mobile SDK — это набор инструментов разработки программного обеспечения, предназначенный для предоставления разработчикам доступа к возможностям беспилотных летательных аппаратов и портативных камер DJI.\nSDK упрощает процесс разработки приложений, беря на себя низкоуровневые функции, такие как стабилизация полета, управление батареей, передача сигналов и связь.\nТаким образом, разработчику не требуется опыт в робототехнике или встроенных системах, и он может сосредоточиться на приложении, в которое интегрирован продукт DJI.\nThe SDK включает:\n    • библиотеку/фреймворк, который можно импортировать в приложение на Android или iOS и который обеспечивает доступ к продукту DJI\n    • симулятор БПЛА и инструмент визуализации\n    • отладчик и удаленный регистратор для iOS\n    • образцы кода и учебники\n    • этот гид разработчика и документацию по API\nЭто введение опишет, как SDK подключается к продуктам, какие возможности предоставляет SDK и базовую архитектуру SDK."
      },
      {
        "model": "grok-4.1-fast",
        "text": "DJI Mobile SDK — это набор средств разработки программного обеспечения, предназначенный для предоставления разработчикам доступа к возможностям воздушных судов и портативных камер DJI. SDK упрощает процесс разработки приложений, беря на себя низкоуровневые функции, такие как стабилизация полета, управление аккумулятором, передача сигнала и связь. Таким образом, разработчику не требуется опыт в робототехнике или встроенных системах, и он может сосредоточиться на приложении, в которое интегрируется продукт DJI.\n\nSDK включает:\n    • библиотеку/фреймворк, которую можно импортировать в приложение для Android или iOS и которая предоставляет доступ к продукту DJI\n    • симулятор воздушного судна и инструмент визуализации\n    • отладчик и удаленный логгер для iOS\n    • примеры кода и руководства\n    • это руководство разработчика и документацию по API\n\nЭто введение опишет, как SDK подключается к продуктам, какие функции предоставляет SDK, и вводную архитектуру SDK."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "DJI Mobile SDK — это комплект для разработки программного обеспечения, предоставляющий разработчикам доступ к возможностям продуктов DJI, таких как летательные аппараты и ручные камеры. SDK упрощает процесс разработки приложений, беря на себя низкоуровневые функции, такие как стабилизация полета, управление батареей, передача сигнала и связь. Таким образом, разработчику не требуется глубоких знаний в области робототехники или встраиваемых систем, и он может сосредоточиться на приложении, в которое интегрируется продукт DJI.\nSDK включает в себя:\n    • библиотеку/фреймворк, который можно импортировать в приложение для Android или iOS и который предоставляет доступ к продукту DJI\n    • авиационный симулятор и инструмент визуализации\n    • отладчик и удаленный регистратор для iOS\n    • примеры кода и учебные пособия\n    • данное руководство разработчика и документацию по API\nВо введении описывается, как SDK подключается к продуктам, какие функции предоставляет SDK и начальная архитектура SDK."
      }
    ]
  },
  {
    "id": 6,
    "original_text": "Advances in artificial intelligence (AI) including large language models (LLMs) and hybrid reasoning models present an opportunity to reimagine how autonomous robots such as drones are designed, developed, and validated. Here, we demonstrate a fully AI-generated drone control system: with minimal human input, an artificial intelligence (AI) model authored all the code for a real-time, self-hosted drone command and control platform, which was deployed and demonstrated on a real drone in flight as well as a simulated virtual drone in the cloud. The system enables real-time mapping, flight telemetry, autonomous mission planning and execution, and safety protocolsall orchestrated through a web interface hosted directly on the drone itself. Not a single line of code was written by a human. We quantitatively benchmark system performance, code complexity, and development speed against prior, human-coded architectures, finding that AI-generated code can deliver functionally complete command-and-control stacks at orders-of-magnitude faster development cycles, though with identifiable current limitations related to specific model context window and reasoning depth. Our analysis uncovers the practical boundaries of AI-driven robot control code generation at current model scales, as well as emergent strengths and failure modes in AI-generated robotics code. This work sets a precedent for the autonomous creation of robot control systems and, more broadly, suggests a new paradigm for robotics engineeringone in which future robots may be largely co-designed, developed, and verified by artificial intelligence. In this initial work, a robot built a robot's brain.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Достижения в области искусственного интеллекта (ИИ), включая большие языковые модели (LLMs) и гибридные модели рассуждений, создают возможность переосмыслить, как автономные роботы, такие как дроны, проектируются, развиваются и проверяются. Здесь мы демонстрируем полностью ИИ-генерированную систему управления дроном: при минимальном участии человека модель искусственного интеллекта (ИИ) написала весь код для платформы управления дроном в реальном времени, которая была развернута и продемонстрирована на реальном дроне в полете, а также на симулированном виртуальном дроне в облаке. Система обеспечивает картографирование в реальном времени, телеметрию полета, автономное планирование миссий и их выполнение, а также протоколы безопасности — всё это управляется через веб-интерфейс, размещённый непосредственно на самом дроне. Ни одной строки кода не писал человек. Мы количественно сравнили производительность системы, сложность кода и скорость разработки с ранее существовавшими архитектурами, реализованными людьми, обнаружив, что код, сгенерированный ИИ, способен обеспечивать функционально полноценные стеки командования и управления на порядки быстрее циклов разработки, хотя существуют явные текущие ограничения, связанные с размером окна контекста модели и глубиной рассуждений. Наш анализ выявляет практические пределы генерации кода управления роботами с использованием ИИ для текущих масштабов моделей, а также возникающие сильные стороны и режимы сбоев в коде робототехники, сгенерированном ИИ. Эта работа устанавливает прецедент автономного создания систем управления роботами и, шире, предполагает новую парадигму робототехнического проектирования — в которой будущие роботы во многом могут быть совместно спроектированы, разработаны и проверены искусственным интеллектом. В этой начальной работе робот построил мозг робота."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Достижения в области искусственного интеллекта (ИИ), включая крупные языковые модели (LLM) и гибридные модели рассуждений, предоставляют возможность переосмыслить, как проектируются, разрабатываются и валидируются автономные роботы, такие как дроны. Здесь мы демонстрируем полностью сгенерированную ИИ систему управления дроном: с минимальным участием человека модель искусственного интеллекта (ИИ) написала весь код для платформы командования и управления дроном в реальном времени с самостоятельным хостингом, которая была развернута и продемонстрирована на реальном дроне в полёте, а также на симулированном виртуальном дроне в облаке. Система обеспечивает картирование в реальном времени, телеметрию полёта, автономное планирование и выполнение миссий, а также протоколы безопасности — всё это оркестрируется через веб-интерфейс, размещённый непосредственно на самом дроне. Ни одна строка кода не была написана человеком. Мы количественно оцениваем производительность системы, сложность кода и скорость разработки по сравнению с предыдущими архитектурами, написанными человеком, и обнаруживаем, что код, сгенерированный ИИ, может обеспечивать функционально полные стеки командования и управления с циклами разработки на порядки быстрее, хотя и с идентифицируемыми текущими ограничениями, связанными с окном контекста конкретной модели и глубиной рассуждений. Наш анализ раскрывает практические границы генерации кода управления роботами с помощью ИИ на текущих масштабах моделей, а также возникающие сильные стороны и режимы отказа в коде робототехники, сгенерированном ИИ. Эта работа устанавливает прецедент для автономного создания систем управления роботами и, в более широком смысле, предлагает новую парадигму инженерии робототехники — в которой будущие роботы могут быть в значительной степени совместно спроектированы, разработаны и верифицированы искусственным интеллектом. В этой начальной работе робот построил мозг робота."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Достижения в области искусственного интеллекта (ИИ), включая большие языковые модели (LLM) и гибридные модели рассуждений, открывают возможность переосмыслить подходы к проектированию, разработке и валидации автономных роботов, таких как дроны. В данной работе мы демонстрируем полностью сгенерированную ИИ систему управления дроном: при минимальном участии человека модель искусственного интеллекта (ИИ) создала весь код для самодостаточной платформы управления и контроля дронами в реальном времени, которая была развернута и продемонстрирована как на реальном дроне в полете, так и на симулированном виртуальном дроне в облаке. Система обеспечивает картирование в реальном времени, телеметрию полета, автономное планирование и выполнение миссий, а также протоколы безопасности — все это оркестрируется через веб-интерфейс, размещенный непосредственно на самом дроне. Ни одна строка кода не была написана человеком. Мы количественно сравниваем производительность системы, сложность кода и скорость разработки с предыдущими, написанными человеком архитектурами, и обнаруживаем, что сгенерированный ИИ код может обеспечивать функционально полные стеки управления и контроля при циклах разработки, ускоренных на порядки, хотя и с выявляемыми текущими ограничениями, связанными с контекстным окном конкретной модели и глубиной рассуждений. Наш анализ выявляет практические границы генерации кода управления роботами на основе ИИ при текущих масштабах моделей, а также возникающие сильные стороны и режимы сбоев в сгенерированном ИИ коде для робототехники. Эта работа создает прецедент для автономного создания систем управления роботами и, в более широком смысле, предполагает новую парадигму для машиностроения в области робототехники — парадигму, в которой будущие роботы могут в значительной степени совместно проектироваться, разрабатываться и верифицироваться искусственным интеллектом. В этой первой работе робот создал \"мозг\" для робота."
      }
    ]
  },
  {
    "id": 7,
    "original_text": "OpenCV has a modular structure, which means that the package includes several shared or static libraries. The following modules are available:\n\n    • Core functionality (core) - a compact module defining basic data structures, including the dense multi-dimensional array Mat and basic functions used by all other modules.\n    • Image Processing (imgproc) - an image processing module that includes linear and non-linear image filtering, geometrical image transformations (resize, affine and perspective warping, generic table-based remapping), color space conversion, histograms, and so on.\n    • Image file reading and writing (imgcodecs) - includes functions for reading and writing image files in various formats.\n    • Video I/O (videoio) - an easy-to-use interface to video capturing and video codecs.\n    • High-level GUI (highgui) - an easy-to-use interface to simple UI capabilities.\n    • Video Analysis (video) - a video analysis module that includes motion estimation, background subtraction, and object tracking algorithms.\n    • Camera Calibration and 3D Reconstruction (calib3d) - basic multiple-view geometry algorithms, single and stereo camera calibration, object pose estimation, stereo correspondence algorithms, and elements of 3D reconstruction.\n    • 2D Features Framework (features2d) - salient feature detectors, descriptors, and descriptor matchers.\n    • Object Detection (objdetect) - detection of objects and instances of the predefined classes (for example, faces, eyes, mugs, people, cars, and so on).\n    • Deep Neural Network module (dnn) - Deep Neural Network module.\n    • Machine Learning (ml) - The Machine Learning module includes a set of classes and functions for statistical classification, regression, and clustering of data.\n    • Computational Photography (photo) - advanced photo processing techniques like denoising, inpainting.\n    • Images stitching (stitching) - functions for image stitching and panorama creation.\n    • ... some other helper modules, such as FLANN and Google test wrappers, Python bindings, and others.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n    • Основная функциональность (core) - компактный модуль, определяющий базовые структуры данных, включая плотный многомерный массив Mat и базовые функции, используемые всеми остальными модулями.\n    • Обработка изображений (imgproc) - модуль обработки изображений, включающий линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (изменение размера, аффинная и перспективная деформация, обобщённое табличное переотображение), конвертацию цветовых пространств, гистограммы и т. д.\n    • Чтение и запись изображений (imgcodecs) - включает функции чтения и записи изображений в различных форматах.\n    • Видео ввод/вывод (videoio) - простой в использовании интерфейс захвата видео и кодеков.\n    • Высокоуровневый GUI (highgui) - простой в использовании интерфейс базовых возможностей пользовательского интерфейса.\n    • Анализ видео (video) - модуль анализа видео, который включает оценку движения, вычитание фона и алгоритмы отслеживания объектов.\n    • Калибровка камер и 3D-восстановление (calib3d) - базовые алгоритмы многократной перспективной геометрии, калибровка одной и стереокамеры, оценка позы объектов, алгоритмы стереосопоставления и элементы 3D-восстановления.\n    • Фреймворк 2D признаков (features2d) - заметные детекторы признаков, дескрипторы и сопоставители дескрипторов.\n    • Обнаружение объектов (objdetect) - обнаружение объектов и экземпляров заданных классов (например, лица, глаза, чашки, люди, автомобили и т. п.).\n    • Модуль глубоких нейронных сетей (dnn) - Модуль глубокой нейронной сети.\n    • Машинное обучение (ml) - Машинный модуль включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n    • Вычислительная фотография (photo) - продвинутые техники обработки фото, такие как удаление шума, заполнение пропусков.\n    • Сшивка изображений (stitching) - функции для сшивания изображений и создания панорам.\n    • ... некоторые другие вспомогательные модули, такие как FLANN и обертки Google Test, Python-привязки и другие."
      },
      {
        "model": "grok-4.1-fast",
        "text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n• Core functionality (core) — компактный модуль, определяющий базовые структуры данных, включая плотный многомерный массив Mat и базовые функции, используемые всеми остальными модулями.\n• Image Processing (imgproc) — модуль обработки изображений, включающий линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (resize, affine и perspective warping, generic table-based remapping), преобразование цветового пространства, гистограммы и т. д.\n• Image file reading and writing (imgcodecs) — включает функции для чтения и записи файлов изображений в различных форматах.\n• Video I/O (videoio) — удобный интерфейс для захвата видео и видеокодеков.\n• High-level GUI (highgui) — удобный интерфейс для простых возможностей UI.\n• Video Analysis (video) — модуль анализа видео, включающий алгоритмы оценки движения, вычитания фона и отслеживания объектов.\n• Camera Calibration and 3D Reconstruction (calib3d) — базовые алгоритмы геометрии с несколькими видами, калибровка одной и стереокамеры, оценка позы объекта, алгоритмы стереосоответствия и элементы 3D-реконструкции.\n• 2D Features Framework (features2d) — детекторы заметных признаков, дескрипторы и матчеры дескрипторов.\n• Object Detection (objdetect) — обнаружение объектов и экземпляров предопределённых классов (например, лица, глаза, кружки, люди, автомобили и т. д.).\n• Deep Neural Network module (dnn) — модуль глубоких нейронных сетей.\n• Machine Learning (ml) — модуль машинного обучения включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n• Computational Photography (photo) — продвинутые техники обработки фотографий, такие как denoising, inpainting.\n• Images stitching (stitching) — функции для сшивания изображений и создания панорам.\n• ... некоторые другие вспомогательные модули, такие как обёртки FLANN и Google test, Python bindings и другие."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "OpenCV имеет модульную структуру, что означает, что пакет включает несколько общих или статических библиотек. Доступны следующие модули:\n\n    • Основная функциональность (core) - компактный модуль, определяющий основные структуры данных, включая плотный многомерный массив Mat и базовые функции, используемые всеми другими модулями.\n    • Обработка изображений (imgproc) - модуль обработки изображений, который включает линейную и нелинейную фильтрацию изображений, геометрические преобразования изображений (изменение размера, аффинное и перспективное преобразование, общая перестановка на основе таблиц), преобразование цветового пространства, гистограммы и т. д.\n    • Чтение и запись файлов изображений (imgcodecs) - включает функции для чтения и записи файлов изображений в различных форматах.\n    • Ввод-вывод видео (videoio) - простой в использовании интерфейс для захвата видео и видеокодеков.\n    • Высокоуровневый GUI (highgui) - простой в использовании интерфейс для базовых возможностей пользовательского интерфейса.\n    • Анализ видео (video) - модуль анализа видео, который включает алгоритмы оценки движения, вычитания фона и отслеживания объектов.\n    • Калибровка камеры и 3D-реконструкция (calib3d) - базовые алгоритмы геометрии нескольких видов, калибровка одиночной и стереокамеры, оценка позы объекта, алгоритмы стереосоответствия и элементы 3D-реконструкции.\n    • Структура 2D-признаков (features2d) - детекторы ярких признаков, дескрипторы и сопоставители дескрипторов.\n    • Обнаружение объектов (objdetect) - обнаружение объектов и экземпляров предопределенных классов (например, лиц, глаз, кружек, людей, автомобилей и т. д.).\n    • Модуль глубоких нейронных сетей (dnn) - модуль глубоких нейронных сетей.\n    • Машинное обучение (ml) - Модуль машинного обучения включает набор классов и функций для статистической классификации, регрессии и кластеризации данных.\n    • Вычислительная фотография (photo) - продвинутые методы обработки фотографий, такие как шумоподавление, восстановление изображений.\n    • Сшивка изображений (stitching) - функции для сшивки изображений и создания панорам.\n    • ... другие вспомогательные модули, такие как обертки FLANN и Google test, привязки Python и другие."
      }
    ]
  },
  {
    "id": 8,
    "original_text": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Обнаружение дронов выиграло от улучшений в глубоких нейронных сетях, но, как и многие другие применения, страдает от доступности точных данных для обучения. Синтетические данные предоставляют потенциал для недорогой генерации данных и показали улучшение доступности и качества данных. Однако модели, обученные на синтетических наборах данных, должны доказать свою способность работать на данных реального мира, известная как проблема переноса между симуляцией и реальностью (sim-to-real). Здесь мы представляем модель Faster-RCNN для обнаружения дронов, обученную исключительно на синтетическом наборе данных, которая переносится на данные реального мира. Мы обнаружили, что она достигает AP_50 равного 97,0% при оценке на MAV-Vid — реальном наборе данных летающих дронов — по сравнению с 97,8% для эквивалентной модели, обученной на данных реального мира. Наши результаты показывают, что использование синтетических данных для обнаружения дронов имеет потенциал снизить затраты на сбор данных и улучшить качество разметки. Эти выводы могут стать отправной точкой для более сложных синтетических наборов данных по дронам. Например, реалистичные воссоздания конкретных сценариев могут снизить риски, связанные с генерацией наборов данных для приложений, критически важных для безопасности, таких как обнаружение дронов в аэропортах. Кроме того, синтетические данные могут обеспечить надёжные системы обнаружения дронов, что может быть полезно в других областях, таких как системы управления беспилотным движением (UTM)."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Обнаружение дронов выиграло от улучшений в глубоких нейронных сетях, но, как и многие другие приложения, страдает от недостаточной доступности точных данных для обучения. Синтетические данные предоставляют потенциал для генерации данных с низкими затратами и были показаны улучшающими доступность и качество данных. Однако модели, обученные на синтетических наборах данных, должны доказать свою способность работать на реальных данных, известную как проблема sim-to-real transferability. Здесь мы представляем модель Faster-RCNN для обнаружения дронов, обученную исключительно на синтетическом наборе данных, которая переносится на реальные данные. Мы обнаружили, что она достигает AP_50 97,0% при оценке на MAV-Vid — реальном наборе данных летающих дронов — по сравнению с 97,8% для эквивалентной модели, обученной на реальных данных. Наши результаты показывают, что использование синтетических данных для обнаружения дронов имеет потенциал для снижения затрат на сбор данных и улучшения качества разметки. Эти выводы могут стать отправной точкой для более сложных синтетических наборов данных дронов. Например, реалистичные реконструкции конкретных сценариев могли бы снизить риски при генерации наборов данных для критически важных приложений, таких как обнаружение дронов в аэропортах. Кроме того, синтетические данные могут сделать возможными надежные системы обнаружения дронов, которые могли бы принести пользу другим областям, таким как системы управления беспилотным трафиком."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Обнаружение дронов получило преимущества благодаря усовершенствованиям глубоких нейронных сетей, но, как и многие другие приложения, страдает от наличия точных данных для обучения. Синтетические данные предоставляют возможность для недорогой генерации данных и, как было показано, улучшают доступность и качество данных. Однако модели, обученные на синтетических наборах данных, должны доказать свою способность работать на реальных данных, что известно как проблема переноса из симуляции в реальность (sim-to-real transferability). Здесь мы представляем модель Faster-RCNN для обнаружения дронов, обученную на чисто синтетическом наборе данных, которая переносится на реальные данные. Мы обнаружили, что она достигает AP_50 равного 97,0% при оценке на MAV-Vid — реальном наборе данных летающих дронов — по сравнению с 97,8% для эквивалентной модели, обученной на реальных данных. Наши результаты показывают, что использование синтетических данных для обнаружения дронов имеет потенциал для снижения затрат на сбор данных и улучшения качества разметки. Эти выводы могут послужить отправной точкой для более сложных синтетических наборов данных для дронов. Например, реалистичное воссоздание конкретных сценариев может снизить риски при генерации наборов данных для критически важных с точки зрения безопасности приложений, таких как обнаружение дронов в аэропортах. Далее, синтетические данные могут обеспечить надежные системы обнаружения дронов, которые могут принести пользу другим областям, таким как системы управления беспилотным движением (unmanned traffic management systems)."
      }
    ]
  },
  {
    "id": 9,
    "original_text": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "Способность БПЛА к эффективной автономной навигации и обходу препятствий в сложных и неизвестных условиях крайне важна для применения в сельскохозяйственном орошении, ликвидации последствий бедствий и логистике.\n\nВ данной работе мы предлагаем навигационный алгоритм DPRL (Distributed Privileged Reinforcement Learning) — end-to-end политику, призванную решить задачу быстрого автономного управления БПЛА в условиях частично наблюдаемой окружающей среды.\n\nНаш подход сочетает глубокое обучение с подкреплением и привилегированное обучение, чтобы преодолеть влияние искажения данных наблюдений, вызванного частичной наблюдаемостью.\n\nМы используем асимметричную архитектуру Actor-Critic, чтобы во время обучения предоставлять агенту привилегированную информацию, что повышает перцептивные возможности модели.\n\nКроме того, мы предлагаем мультиагентную стратегию исследования в разнообразных средах, чтобы ускорить сбор опыта, что, в свою очередь, ускоряет сходимость модели.\n\nМы провели обширные симуляции в различных сценариях, сопоставив наш алгоритм DPRL с передовыми алгоритмами навигации.\n\nРезультаты последовательно демонстрируют превосходство нашего алгоритма по таким критериям, как эффективность полета, устойчивость и общий показатель успешности."
      },
      {
        "model": "grok-4.1-fast",
        "text": "Способность БПЛА к эффективной автономной навигации и избеганию препятствий в сложных и неизвестных средах критически важна для приложений в сельскохозяйственном орошении, ликвидации последствий стихийных бедствий и логистике. В этой работе мы предлагаем алгоритм навигации DPRL (Distributed Privileged Reinforcement Learning) — политику end-to-end, предназначенную для решения задачи высокоскоростной автономной навигации БПЛА в условиях частично наблюдаемой среды. Наш подход сочетает глубокое обучение с подкреплением с привилегированным обучением, чтобы преодолеть влияние искажения данных наблюдений, вызванного частичной наблюдаемостью. Мы используем асимметричную архитектуру Actor-Critic, чтобы предоставить агенту привилегированную информацию во время обучения, что повышает перцептивные возможности модели. Кроме того, мы представляем стратегию исследования с несколькими агентами в разнообразных средах для ускорения сбора опыта, что, в свою очередь, ускоряет сходимость модели. Мы провели обширные симуляции в различных сценариях, сравнивая наш алгоритм DPRL с передовыми алгоритмами навигации. Результаты последовательно демонстрируют превосходную производительность нашего алгоритма по показателям эффективности полета, надежности и общей успешности."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "Возможности БПЛА для эффективной автономной навигации и обхода препятствий в сложных и неизвестных средах имеют решающее значение для применения в сельскохозяйственном орошении, ликвидации последствий стихийных бедствий и логистике. В данной статье мы предлагаем алгоритм навигации DPRL (Distributed Privileged Reinforcement Learning) — модель сквозного управления, разработанную для решения задачи высокоскоростной автономной навигации БПЛА в условиях частичной наблюдаемости окружающей среды. Наш подход объединяет глубокое обучение с подкреплением и привилегированное обучение для преодоления влияния искажения данных наблюдений, вызванного частичной наблюдаемостью. Мы используем асимметричную архитектуру Actor-Critic, чтобы предоставить агенту привилегированную информацию во время обучения, что улучшает перцепционные возможности модели. Кроме того, мы представляем стратегию исследования на основе множества агентов в разнообразных средах для ускорения сбора опыта, что, в свою очередь, ускоряет сходимость модели. Мы провели обширные симуляции в различных сценариях, сравнивая наш алгоритм DPRL с современными навигационными алгоритмами. Результаты последовательно демонстрируют превосходную производительность нашего алгоритма с точки зрения эффективности полета, робастности и общего уровня успеха."
      }
    ]
  },
  {
    "id": 10,
    "original_text": "In recent years, the increasing use of drones for both commercial and recreational purposes has led to heightened concerns regarding airspace safety. To address these issues, machine learning (ML) based drone detection and classification have emerged. This study explores the potential of ML-based drone classification, utilizing technologies like radar, visual, acoustic, and radio-frequency sensing systems. It undertakes a comprehensive examination of the existing literature in this domain, with a focus on various sensing modalities and their respective technological implementations. The study indicates that ML-based drone classification is promising, with numerous successful individual contributions. It is crucial to note, however, that much of the research in this field is experimental, making it difficult to compare results from various articles. There is also a noteworthy lack of reference datasets to help in the evaluation of different solutions.",
    "translations": [
      {
        "model": "gpt-5",
        "text": "В последние годы растущее использование беспилотников как в коммерческих, так и в бытовых целях привело к повышенной обеспокоенности безопасностью воздушного пространства. Чтобы справиться с этими вопросами, появились подходы к обнаружению и классификации беспилотников на основе машинного обучения (ML). Это исследование изучает потенциал классификации беспилотников на основе ML, используя такие технологии, как радиолокационные, визуальные, акустические и радиочастотные сенсорные системы. Оно проводит комплексный обзор существующей литературы в этой области, с акцентом на различные модальности сенсирования и их соответствующие технологические реализации. Исследование свидетельствует, что классификация беспилотников на основе ML выглядит многообещающей, с многочисленными успешными отдельными вкладами. Однако важно отметить, что значительная часть исследований в этой области носит экспериментальный характер, что затрудняет сравнение результатов между различными публикациями. Также отмечается заметная нехватка эталонных наборов данных, которые помогли бы в оценке различных решений."
      },
      {
        "model": "grok-4.1-fast",
        "text": "В последние годы возрастающее использование дронов в коммерческих и рекреационных целях привело к усилению опасений по поводу безопасности воздушного пространства. Для решения этих проблем появились методы обнаружения и классификации дронов на основе machine learning (ML). Это исследование изучает потенциал классификации дронов на основе ML с использованием технологий, таких как radar, visual, acoustic и radio-frequency sensing systems. Оно проводит всесторонний обзор существующей литературы в этой области с акцентом на различные sensing modalities и их соответствующие технологические реализации. Исследование показывает, что классификация дронов на основе ML перспективна и имеет многочисленные успешные индивидуальные вклады. Однако важно отметить, что большая часть исследований в этой области экспериментальная, что затрудняет сравнение результатов из различных статей. Также примечательно отсутствие reference datasets для оценки различных решений."
      },
      {
        "model": "gemini-2.0-flash-lite",
        "text": "В последние годы увеличение использования дронов как в коммерческих, так и в рекреационных целях вызвало повышенную озабоченность по поводу безопасности воздушного пространства. Для решения этих проблем появились системы обнаружения и классификации дронов на основе машинного обучения (ML). Данное исследование изучает потенциал классификации дронов на основе ML с использованием таких технологий, как радарные, визуальные, акустические и радиочастотные сенсорные системы. В нем проводится комплексный анализ существующей литературы в этой области с акцентом на различные сенсорные модальности и их соответствующие технологические реализации. Исследование показывает, что классификация дронов на основе ML является перспективной, с многочисленными успешными индивидуальными вкладами. Однако важно отметить, что большая часть исследований в этой области носит экспериментальный характер, что затрудняет сравнение результатов из различных статей. Также отмечается заметная нехватка эталонных наборов данных для оценки различных решений."
      }
    ]
  }
]